{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish0045h/ai_news/blob/main/ai_news(BRET).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mal6vaj9Vzk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxbjy_p89Y4L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def load_model_and_tokenizer(model_name=\"facebook/bart-large-cnn\"):\n",
        "    \"\"\"Loads the BART model and tokenizer for summarization.\"\"\"\n",
        "    print(f\"Loading model and tokenizer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmUhxFBeq-6j"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def load_and_prepare_dataset_for_bart(tokenizer, train_samples=2000, eval_samples=500):\n",
        "    print(\"Loading CNN/DailyMail dataset...\")\n",
        "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "    # Select subset for faster training or debugging\n",
        "    train_dataset = dataset[\"train\"].select(range(train_samples))\n",
        "    eval_dataset = dataset[\"validation\"].select(range(eval_samples))\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        \"\"\"Tokenize input (article) and target (highlights) for BART.\"\"\"\n",
        "        # No prefix needed for BART\n",
        "        inputs = examples[\"article\"]\n",
        "        model_inputs = tokenizer(\n",
        "            inputs,\n",
        "            max_length=1024,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        # Tokenize summaries (targets)\n",
        "        labels = tokenizer(\n",
        "            text_target=examples[\"highlights\"],\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "    tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
        "\n",
        "    return tokenized_train_dataset, tokenized_eval_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BVFdGkXvRgc"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "def get_training_arguments(output_dir=\"./results_bart_summarizer\"):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,                   # ✅ BART usually converges faster (3–4 epochs is enough)\n",
        "        per_device_train_batch_size=4,        # ✅ Slightly lower to prevent GPU OOM on Colab\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=2,        # ✅ Effectively doubles batch size\n",
        "        learning_rate=2e-5,                   # ✅ Slightly smaller LR works better for BART\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",          # ✅ Corrected keyword argument name from 'evaluation_strategy' to 'eval_strategy'\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,                   # ✅ Keeps only 2 best checkpoints\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",         # ✅ Select best model by lowest loss\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\",                     # Disable wandb/tensorboard unless needed\n",
        "        fp16=True,                            # ✅ Mixed precision for Colab GPU\n",
        "        dataloader_num_workers=2,             # ✅ Speeds up data loading\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94duvWVgyIyB"
      },
      "outputs": [],
      "source": [
        "def train_model(model, tokenizer, train_dataset, eval_dataset, training_args):\n",
        "    print(\"Starting fine-tuning on BART...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.train()\n",
        "    print(\"Training complete!\")\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mowg5nSKyd-p"
      },
      "outputs": [],
      "source": [
        "def save_and_test_model(trainer, test_article, save_path=\"./my_bart_summarizer\"):\n",
        "    # Save the fine-tuned model\n",
        "    trainer.save_model(save_path)\n",
        "    print(f\"✅ Model saved to: {save_path}\")\n",
        "\n",
        "    # Load summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=save_path, tokenizer=save_path)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = summarizer(\n",
        "        test_article,\n",
        "        max_length=150,   # controls length of summary\n",
        "        min_length=30,    # prevents too-short summaries\n",
        "        do_sample=False   # deterministic output (optional)\n",
        "    )\n",
        "\n",
        "    # Display result\n",
        "    print(\"\\n--- TEST ARTICLE ---\")\n",
        "    print(test_article[:500], \"...\")  # print only part of article\n",
        "    print(\"\\n--- GENERATED SUMMARY ---\")\n",
        "    print(summary[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "RFyXpIzJypMY",
        "outputId": "d3118443-34e5-45e4-f722-a583c3f88c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer: facebook/bart-large-cnn\n",
            "Loading CNN/DailyMail dataset...\n",
            "Tokenizing dataset...\n",
            "Starting fine-tuning on BART...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1935627403.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 32:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.044500</td>\n",
              "      <td>0.638992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.519800</td>\n",
              "      <td>0.619548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.354000</td>\n",
              "      <td>0.684747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.222100</td>\n",
              "      <td>0.764771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training complete!\n",
            "✅ Model saved to: ./my_bart_summarizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Your max_length is set to 150, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TEST ARTICLE ---\n",
            "\n",
            "    The Karnataka Forest Department has initiated a new conservation program in the forests surrounding Sirsi\n",
            "    to protect the Malabar pied hornbill. The program involves local communities in monitoring nesting sites\n",
            "    and preventing illegal logging. Officials stated on Friday that this collaborative effort aims to ensure\n",
            "    the long-term survival of the iconic bird species, which is crucial for the region's biodiversity.\n",
            "    The initiative also includes awareness campaigns in local school ...\n",
            "\n",
            "--- GENERATED SUMMARY ---\n",
            "Karnataka Forest Department has initiated a new conservation program in the forests surrounding Sirsi .\n",
            "The program involves local communities in monitoring nesting sites and preventing illegal logging .\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Use a BART model for better summarization quality\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "    # Load and prepare CNN/DailyMail dataset\n",
        "    train_dataset, eval_dataset = load_and_prepare_dataset_for_bart(tokenizer)\n",
        "\n",
        "    # Get optimized training arguments for GPU\n",
        "    training_args = get_training_arguments()\n",
        "\n",
        "    # Train the model\n",
        "    trainer = train_model(model, tokenizer, train_dataset, eval_dataset, training_args)\n",
        "\n",
        "    # Example test article\n",
        "    test_article = \"\"\"\n",
        "    The Karnataka Forest Department has initiated a new conservation program in the forests surrounding Sirsi\n",
        "    to protect the Malabar pied hornbill. The program involves local communities in monitoring nesting sites\n",
        "    and preventing illegal logging. Officials stated on Friday that this collaborative effort aims to ensure\n",
        "    the long-term survival of the iconic bird species, which is crucial for the region's biodiversity.\n",
        "    The initiative also includes awareness campaigns in local schools.\n",
        "    \"\"\"\n",
        "\n",
        "    # Save and test\n",
        "    new_save_path = \"./my_bart_summarizer\"\n",
        "    save_and_test_model(trainer, test_article, save_path=new_save_path)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lrqk2cRhywwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f66d0e-4653-40c7-d9eb-687ececb08e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmodel.safetensors\t tokenizer.json\n",
            "generation_config.json\tspecial_tokens_map.json  training_args.bin\n",
            "merges.txt\t\ttokenizer_config.json\t vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls ./my_bart_summarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iiBuuc_8_F3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ca182b01",
        "outputId": "fcabf555-ff69-4450-f4e5-269f449ef6ee"
      },
      "source": [
        "# Zip the saved model directory\n",
        "!zip -r my_bart_summarizer.zip ./my_bart_summarizer\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('my_bart_summarizer.zip')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: my_bart_summarizer/ (stored 0%)\n",
            "  adding: my_bart_summarizer/tokenizer_config.json (deflated 75%)\n",
            "  adding: my_bart_summarizer/vocab.json (deflated 59%)\n",
            "  adding: my_bart_summarizer/generation_config.json (deflated 46%)\n",
            "  adding: my_bart_summarizer/special_tokens_map.json (deflated 52%)\n",
            "  adding: my_bart_summarizer/training_args.bin (deflated 53%)\n",
            "  adding: my_bart_summarizer/tokenizer.json (deflated 82%)\n",
            "  adding: my_bart_summarizer/config.json (deflated 62%)\n",
            "  adding: my_bart_summarizer/merges.txt (deflated 53%)\n",
            "  adding: my_bart_summarizer/model.safetensors (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_272044ba-4e48-4f0f-aa37-e65ecc416a06\", \"my_bart_summarizer.zip\", 1508096839)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xf5_weAi_cZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOUZrZAr4+1HoyPFi2q6KXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}